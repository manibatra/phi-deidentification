{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef590e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/phi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09de8128-1fb2-4b32-aa8e-f924874769a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff69b58-bcda-463a-a2cf-9a9cd3812d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5591e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = './i2b2_parsing/parsed_gold_set_1.jsonl'\n",
    "test_path = './i2b2_parsing/parsed_test.jsonl'\n",
    "data = pd.read_json(file_path_1, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25780753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fef6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nRecord date: 2080-11-30\\n\\n\\n\\nReason for Visit\\n\\nOwen is a 63 y/o male here for evaluation of treatment. Doin relatively well. \\n\\n\\n\\nProblems\\n\\n\\n\\n      OA\\n\\n\\n\\n      LLE-PARTIALLY SEVERED-MULT. SURGERIES\\n\\n\\n\\n      IRRIDECTOMY\\n\\n\\n\\n      SKIN ULCER-DR Esposito\\n\\n\\n\\n      PAST SMOKER \\n\\n\\n\\n      HTN\\n\\n\\n\\n\\n\\nMedications\\n\\n\\n\\n      ASA       PO \\n\\n\\n\\n      Vitamin E        PO QD : 400 IU \\n\\n\\n\\n      ATENOLOL   25 MG PO QD\\n\\n\\n\\n      Lipitor (ATORVASTATIN)    10MG,  1 Tablet(s)  PO QD\\n\\n\\n\\n\\n\\nAllergies\\n\\n\\n\\n      NKDA    - NONE\\n\\n\\n\\n\\n\\nNarrative History\\n\\nTakes meds. No SEs. Denies vision change, headache, chest pain, SOB, light head, palptations. Denies loss of balance, strength or sensation. \\n\\n Pulm- no cough.  Occ wheeze. No SOB.\\n\\nGI- no nausea, vomitting, dyspepsia, reflux, abdo pain, diarrhea, constipation, melena, BRBPR.\\n\\nGU- asymp\\n\\nLocomotor- pain left knee/leg. \\n\\nSees Dr Esposito for chr ulcer. Has surgical boot on now. \\n\\nExercise- no\\n\\nDiet-no\\n\\nCigs-no\\n\\nETOH-no\\n\\n\\n\\nExam\\n\\nBP=132/76 , P= 68, Wt= 261 ; NAD,WD, WN\\n\\nHead- no tenderness\\n\\nM&T- moist; no erythema; no exudate; no lesions\\n\\nNeck- supple with no JVD, bruit, LAN or thyromegaly.\\n\\nChest- clear A&P                                                                   Cor- reg rhythm,S1S2 normal with no murmer, gallop or rub\\n\\nAbdo- obese; normal BS;soft with no HSM,mass or tenderness.  DRE- normal sphincter. Prostate small with no nodules. Brown stool.\\n\\nExt- Surg boot LLE. RLE-no edema. \\n\\n\\n\\nAssessment\\n\\nNormotensive. Increased weight. Did have elevated glucose last visit. \\n\\n\\n\\nDisposition and Plans\\n\\nCBC,glu,PSA. Decrease weight- increase exercise and eat less. Cont meds. RTC 6 mon or PRN.\\n\\n______________________________\\n\\n William Seth Potter, M.D.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data['text'].tolist() # type: ignore\n",
    "test_texts = test_data['text'].tolist() # type: ignore\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d14d702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'P0',\n",
       "  'start': '16',\n",
       "  'end': '26',\n",
       "  'text': '2080-11-30',\n",
       "  'TYPE': 'DATE',\n",
       "  'comment': '',\n",
       "  'label': 'DATE'},\n",
       " {'id': 'P1',\n",
       "  'start': '48',\n",
       "  'end': '52',\n",
       "  'text': 'Owen',\n",
       "  'TYPE': 'PATIENT',\n",
       "  'comment': '',\n",
       "  'label': 'PATIENT'},\n",
       " {'id': 'P2',\n",
       "  'start': '58',\n",
       "  'end': '60',\n",
       "  'text': '63',\n",
       "  'TYPE': 'AGE',\n",
       "  'comment': '',\n",
       "  'label': 'AGE'},\n",
       " {'id': 'P3',\n",
       "  'start': '242',\n",
       "  'end': '250',\n",
       "  'text': 'Esposito',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'},\n",
       " {'id': 'P4',\n",
       "  'start': '854',\n",
       "  'end': '862',\n",
       "  'text': 'Esposito',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'},\n",
       " {'id': 'P5',\n",
       "  'start': '1664',\n",
       "  'end': '1683',\n",
       "  'text': 'William Seth Potter',\n",
       "  'TYPE': 'DOCTOR',\n",
       "  'comment': '',\n",
       "  'label': 'STAFF'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans = data['spans'].tolist() # type: ignore\n",
    "test_spans = test_data['spans'].tolist() # type: ignore\n",
    "spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70cbf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87e6e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/mambaforge/envs/phi/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, convert_slow_tokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d73d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_function(x): return tokz(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(data)\n",
    "test_ds = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf4b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'spans', 'meta', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 790\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_function, batched=True)\n",
    "test_tok_ds = test_ds.map(tok_function, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913a464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_ds['text'][0]\n",
    "# tok_ds['input_ids'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656e01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "278a0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tok_ds['input_ids'][0]\n",
    "# tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "# token_positions = tokz(tok_ds['text'][0], return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "# for token, input_id, token_pos in zip(tokens, input_ids, token_positions):\n",
    "#     print(f'{token}: {input_id}: {token_pos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58251d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"Padding\": -100,\n",
    "    \"O\": 0,\n",
    "    \"B-DATE\": 1,\n",
    "    \"I-DATE\": 2,\n",
    "    \"B-PATIENT\": 3,\n",
    "    \"I-PATIENT\": 4,\n",
    "    \"B-AGE\": 5,\n",
    "    \"I-AGE\": 6,\n",
    "    \"B-STAFF\": 7,\n",
    "    \"I-STAFF\": 8,\n",
    "    \"B-PHONE\": 9,\n",
    "    \"I-PHONE\": 10,\n",
    "    \"B-EMAIL\": 11,\n",
    "    \"I-EMAIL\": 12,\n",
    "    \"B-ID\": 13,\n",
    "    \"I-ID\": 14,\n",
    "    \"B-HOSP\": 15,\n",
    "    \"I-HOSP\": 16,\n",
    "    \"B-PATORG\": 17,\n",
    "    \"I-PATORG\": 18,\n",
    "    \"B-LOC\": 19,\n",
    "    \"I-LOC\": 20,\n",
    "    \"B-OTHERPHI\": 21,\n",
    "    \"I-OTHERPHI\": 22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(text, spans, input_ids):\n",
    "    tokens = tokz.convert_ids_to_tokens(input_ids)\n",
    "    labels = ['O'] * len(input_ids)\n",
    "    token_positions = tokz(text, return_offsets_mapping=True)['offset_mapping']\n",
    "\n",
    "\n",
    "    for span in spans:\n",
    "        start, end, label = span['start'], span['end'], span['label']\n",
    "\n",
    "        token_start, token_end = None, None\n",
    "\n",
    "        for idx, (char_start, char_end) in enumerate(token_positions):\n",
    "            if tokens[idx].startswith('▁'):\n",
    "                char_start += 1\n",
    "            # print(tokens[idx], char_start, char_end, start, end)\n",
    "            if char_start == int(start):\n",
    "                token_start = idx\n",
    "            if char_end == int(end):\n",
    "                token_end = idx\n",
    "                break\n",
    "        \n",
    "        if token_start is not None and token_end is not None:\n",
    "            # print(token_start, token_end, label)\n",
    "            labels[token_start] = f'B-{label}'\n",
    "            for idx in range(token_start + 1, token_end + 1):\n",
    "                labels[idx] = f'I-{label}'\n",
    "\n",
    "    # input_ids = tokz.convert_tokens_to_ids(tokens)\n",
    "    label_ids = [label_map[label] for label in labels]\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988c19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(tok_ds['text'], tok_ds['spans'], tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615b5593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4995"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_len(x):\n",
    "    return max([len(input_ids) for input_ids in x['input_ids']])\n",
    "\n",
    "max_len = find_max_len(tok_ds_processed)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d520990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(sequences, max_length, padding_value):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_seq = seq[:max_length]\n",
    "        else:\n",
    "            padded_seq = seq + [padding_value] * (max_length - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81798b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_data(tok_ds_processed['input_ids'], 1024, 0)\n",
    "labels = pad_data(tok_ds_processed['labels'], 1024, -100)\n",
    "attention_mask = pad_data(tok_ds_processed['attention_mask'], 1024, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98fa6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds_processed.remove_columns(['input_ids', 'labels', 'attention_mask', 'text', 'spans', 'meta'])\n",
    "tok_ds_processed = tok_ds_processed.add_column('input_ids', input_ids)\n",
    "tok_ds_processed = tok_ds_processed.add_column('labels', labels)\n",
    "tok_ds_processed = tok_ds_processed.add_column('attention_mask', attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a25153d-f065-4ff7-9019-0bba106a37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_processed = tok_ds_processed.remove_columns(['token_type_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c364441",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok_ds_processed = test_tok_ds.add_column('labels', [pre_process_data(text, spans, input_ids) for text, spans, input_ids in zip(test_tok_ds['text'], test_tok_ds['spans'], test_tok_ds['input_ids'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68e8e13a-40dc-4e99-8810-c58b99c740e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = pad_data(test_tok_ds_processed['input_ids'], 1024, 0)\n",
    "test_labels = pad_data(test_tok_ds_processed['labels'], 1024, -100)\n",
    "test_attention_mask = pad_data(test_tok_ds_processed['attention_mask'], 1024, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3f02f47-be6a-4353-9022-7ad52cad1340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tok_ds_processed = test_tok_ds_processed.remove_columns(['input_ids', 'labels', 'attention_mask', 'text', 'spans', 'meta'])\n",
    "test_tok_ds_processed = test_tok_ds_processed.add_column('input_ids', test_input_ids)\n",
    "test_tok_ds_processed = test_tok_ds_processed.add_column('labels', test_labels)\n",
    "test_tok_ds_processed = test_tok_ds_processed.add_column('attention_mask', test_attention_mask)\n",
    "test_tok_ds_processed = test_tok_ds_processed.remove_columns(['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4cf791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print out mapping of the tokens to the labels\n",
    "# tokens = tokz.convert_ids_to_tokens(tok_ds_processed['input_ids'][1])\n",
    "# labels = [list(label_map.keys())[list(label_map.values()).index(label_id)] for label_id in tok_ds_processed['labels'][1]]\n",
    "\n",
    "# for token, label in zip(tokens, labels):\n",
    "#     print(f'{token} - {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86107c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels', 'attention_mask'],\n",
       "        num_rows: 592\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels', 'attention_mask'],\n",
       "        num_rows: 198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create test and validation sets\n",
    "## test_tok_ds_processed is the test set\n",
    "tok_dds = tok_ds_processed.train_test_split(test_size=0.25, seed=42)\n",
    "tok_dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1be2f10-a5f1-4717-b331-ed291be6f5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 258 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f76fa",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "268b76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bbc88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd09f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine',\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none', gradient_accumulation_steps=4, optim=\"adafactor\", gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2aeb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88bd1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_nm, num_labels=len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef43ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f259649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_nm, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6feae9-46b3-45da-abfc-068da4eb36db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 261 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a219f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, args, train_dataset=tok_dds['train'], eval_dataset=tok_dds['test'], tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9a5da8a-5408-46dd-adbc-9c4192963be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.backends.cudnn.benchmark=True\n",
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a681a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='592' max='592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [592/592 07:18, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.015754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.009772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.007673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.007552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b751192-e77a-4557-8fc8-cb4c793d7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 440.05\n",
      "Samples/second: 5.38\n",
      "GPU memory occupied: 3484 MB.\n"
     ]
    }
   ],
   "source": [
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb224396-2d8e-45a4-a9d1-ee5d39c5328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/storage/models/phi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3e783ee-7260-427c-9eb7-9427d487bf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_result = trainer.predict(test_tok_ds_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a44a8e0-2fd0-4e1b-8bad-8ec7ccddd67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "▁Record - O\n",
      "▁date - O\n",
      ": - O\n",
      "▁20 - B-DATE\n",
      "90 - I-DATE\n",
      "- - I-DATE\n",
      "07 - I-DATE\n",
      "- - I-DATE\n",
      "16 - I-DATE\n",
      "▁NAME - O\n",
      ": - O\n",
      "▁Curtis - B-PATIENT\n",
      ", - I-PATIENT\n",
      "▁Om - I-PATIENT\n",
      "▁M - O\n",
      "RN - O\n",
      ": - O\n",
      "▁768 - B-ID\n",
      "294 - I-ID\n",
      "1 - I-ID\n",
      "▁He - O\n",
      "▁is - O\n",
      "▁feeling - O\n",
      "▁great - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁is - O\n",
      "▁all - O\n",
      "▁done - O\n",
      "▁with - O\n",
      "▁his - O\n",
      "▁radiation - O\n",
      "▁to - O\n",
      "▁the - O\n",
      "▁left - O\n",
      "▁a - O\n",
      "x - O\n",
      "illa - O\n",
      "▁for - O\n",
      "▁metastatic - O\n",
      "▁squamous - O\n",
      "▁cell - O\n",
      "▁cancer - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁is - O\n",
      "▁following - O\n",
      "▁closely - O\n",
      "▁with - O\n",
      "▁the - O\n",
      "▁radiation - O\n",
      "▁oncologist - O\n",
      "▁and - O\n",
      "▁the - O\n",
      "▁medical - O\n",
      "▁oncologist - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁is - O\n",
      "▁seeing - O\n",
      "▁them - O\n",
      "▁both - O\n",
      "▁later - O\n",
      "▁this - O\n",
      "▁month - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁has - O\n",
      "▁had - O\n",
      "▁no - O\n",
      "▁problems - O\n",
      "▁with - O\n",
      "▁chest - O\n",
      "▁pains - O\n",
      "▁or - O\n",
      "▁shortness - O\n",
      "▁of - O\n",
      "▁breath - O\n",
      ". - O\n",
      "▁All - O\n",
      "▁in - O\n",
      "▁all - O\n",
      ", - O\n",
      "▁things - O\n",
      "▁are - O\n",
      "▁going - O\n",
      "▁well - O\n",
      ". - O\n",
      "▁PHYSICAL - O\n",
      "▁EXAM - O\n",
      ": - O\n",
      "▁On - O\n",
      "▁exam - O\n",
      ", - O\n",
      "▁no - O\n",
      "▁acute - O\n",
      "▁distress - O\n",
      ". - O\n",
      "▁Lung - O\n",
      "s - O\n",
      "▁are - O\n",
      "▁clear - O\n",
      ". - O\n",
      "▁Heart - O\n",
      "▁is - O\n",
      "▁regular - O\n",
      "▁rate - O\n",
      "▁and - O\n",
      "▁rhythm - O\n",
      ". - O\n",
      "▁No - O\n",
      "▁murmurs - O\n",
      ", - O\n",
      "▁gallop - O\n",
      "s - O\n",
      "▁or - O\n",
      "▁rubs - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁does - O\n",
      "▁have - O\n",
      "▁some - O\n",
      "▁skin - O\n",
      "▁discoloration - O\n",
      "▁around - O\n",
      "▁the - O\n",
      "▁left - O\n",
      "▁a - O\n",
      "x - O\n",
      "illa - O\n",
      "▁but - O\n",
      "▁I - O\n",
      "▁feel - O\n",
      "▁no - O\n",
      "▁mass - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁has - O\n",
      "▁a - O\n",
      "▁well - O\n",
      "- - O\n",
      "heal - O\n",
      "ed - O\n",
      "▁incision - O\n",
      ". - O\n",
      "▁There - O\n",
      "▁is - O\n",
      "▁no - O\n",
      "▁hair - O\n",
      "▁noted - O\n",
      "▁in - O\n",
      "▁or - O\n",
      "▁around - O\n",
      "▁the - O\n",
      "▁a - O\n",
      "x - O\n",
      "illa - O\n",
      ". - O\n",
      "▁Extrem - O\n",
      "ities - O\n",
      "▁with - O\n",
      "▁no - O\n",
      "▁edema - O\n",
      ". - O\n",
      "▁AS - O\n",
      "S - O\n",
      "ESS - O\n",
      "MENT - O\n",
      "▁AND - O\n",
      "▁PLAN - O\n",
      ": - O\n",
      "▁( - O\n",
      "1 - O\n",
      ") - O\n",
      "▁CAD - O\n",
      "/ - O\n",
      "hypertension - O\n",
      "/ - O\n",
      "diabetes - O\n",
      "▁mellitus - O\n",
      ". - O\n",
      "▁This - O\n",
      "▁is - O\n",
      "▁stable - O\n",
      ". - O\n",
      "▁Check - O\n",
      "▁glycosyl - O\n",
      "ated - O\n",
      "▁hemoglobin - O\n",
      ". - O\n",
      "▁( - O\n",
      "2 - O\n",
      ") - O\n",
      "▁Meta - O\n",
      "static - O\n",
      "▁squamous - O\n",
      "▁cell - O\n",
      "▁cancer - O\n",
      ". - O\n",
      "▁He - O\n",
      "▁is - O\n",
      "▁being - O\n",
      "▁followed - O\n",
      "▁closely - O\n",
      "▁by - O\n",
      "▁Oncology - O\n",
      "▁for - O\n",
      "▁this - O\n",
      ". - O\n",
      "▁Follow - O\n",
      "- - O\n",
      "up - O\n",
      "▁with - O\n",
      "▁me - O\n",
      "▁in - O\n",
      "▁the - O\n",
      "▁spring - B-DATE\n",
      ". - O\n",
      "▁William - B-STAFF\n",
      "▁V - I-STAFF\n",
      ". - I-STAFF\n",
      "▁Geiger - I-STAFF\n",
      ", - O\n",
      "▁M - O\n",
      ". - O\n",
      "D - O\n",
      ". - O\n",
      "▁WV - B-STAFF\n",
      "G - I-STAFF\n",
      "/ - O\n",
      "xin - B-STAFF\n",
      "/ - O\n",
      "quil - B-STAFF\n",
      "ici - I-STAFF\n",
      "[SEP] - O\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n",
      "[PAD] - Padding\n"
     ]
    }
   ],
   "source": [
    "# # Print out mapping of the tokens to the labels\n",
    "tokens = tokz.convert_ids_to_tokens(test_tok_ds_processed['input_ids'][0])\n",
    "labels = [list(label_map.keys())[list(label_map.values()).index(label_id)] for label_id in prediction_result.label_ids[0]]\n",
    "\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f'{token} - {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48abe35-b695-4434-8c88-d5e80b1f161f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi",
   "language": "python",
   "name": "phi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
